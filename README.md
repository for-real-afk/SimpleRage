# Production RAG API

A production-ready Retrieval-Augmented Generation (RAG) API built with FastAPI, optimized for deployment on Render's free tier. Upload documents, ask questions, and get AI-powered answers based on your knowledge base.

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## ğŸŒŸ Features

- **ğŸ“„ Document Processing**: Upload `.txt`, `.pdf`, and `.docx` files
- **ğŸ” Semantic Search**: Find relevant information using vector embeddings
- **ğŸ¤– AI-Powered Answers**: Generate contextual responses using Google's Gemini
- **âš¡ Production-Ready**: Rate limiting, error handling, logging, and timeouts
- **ğŸ’° Free-Tier Optimized**: Designed to run on Render's 512MB RAM free tier
- **ğŸ”’ Secure**: CORS configuration, input validation, and environment-based secrets
- **ğŸ“Š Monitoring**: Health checks and comprehensive logging

## ğŸ“‹ Table of Contents

- [Architecture](#-architecture)
- [How It Works](#-how-it-works)
- [Quick Start](#-quick-start)
- [API Documentation](#-api-documentation)
- [Deployment](#-deployment)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)
- [Performance](#-performance)

## ğŸ—ï¸ Architecture

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT / USER                            â”‚
â”‚                    (Web, Mobile, API Client)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ HTTP Requests
                             â”‚ (REST API)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FASTAPI APPLICATION                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Middleware Layer                       â”‚   â”‚
â”‚  â”‚  â€¢ CORS Handler                                          â”‚   â”‚
â”‚  â”‚  â€¢ Rate Limiter (SlowAPI)                                â”‚   â”‚
â”‚  â”‚  â€¢ Error Handler                                         â”‚   â”‚
â”‚  â”‚  â€¢ Request Logger                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              API Endpoints (Routes)                       â”‚  â”‚
â”‚  â”‚                          â”‚                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚  â”‚           â”‚          â”‚           â”‚             â”‚      â”‚  â”‚
â”‚  â”‚  â”‚  POST     â”‚  POST    â”‚   GET     â”‚   DELETE    â”‚      â”‚  â”‚
â”‚  â”‚  â”‚ /upload   â”‚ /query   â”‚ /health   â”‚  /clear     â”‚      â”‚  â”‚
â”‚  â”‚  â”‚           â”‚          â”‚           â”‚             â”‚      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚        â”‚          â”‚           â”‚            â”‚             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚          â”‚           â”‚            â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Business Logic Layer                         â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  â€¢ Document Processing (extract_text)                     â”‚  â”‚
â”‚  â”‚  â€¢ Text Chunking (chunk_text)                             â”‚  â”‚
â”‚  â”‚  â€¢ Async Embedding Generation (get_embedding_async)       â”‚  â”‚
â”‚  â”‚  â€¢ Async Answer Generation (generate_answer_async)        â”‚  â”‚
â”‚  â”‚  â€¢ Vector Batch Upload (upsert_vectors_batch)             â”‚  â”‚
â”‚  â”‚  â€¢ Timeout & Retry Logic                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                         â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚       â”‚                  â”‚
         â”‚  PINECONE API   â”‚       â”‚   GEMINI API     â”‚
         â”‚  (Vector DB)    â”‚       â”‚  (Google AI)     â”‚
         â”‚                 â”‚       â”‚                  â”‚
         â”‚  â€¢ Store        â”‚       â”‚  â€¢ Embeddings    â”‚
         â”‚  â€¢ Search       â”‚       â”‚    (3072-dim)    â”‚
         â”‚  â€¢ Retrieve     â”‚       â”‚  â€¢ Text Gen      â”‚
         â”‚    Vectors      â”‚       â”‚    (Gemini 2.5)  â”‚
         â”‚                 â”‚       â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

#### 1. Document Upload Flow

```
User uploads document
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Validate file     â”‚
â”‚  â€¢ Type check      â”‚
â”‚  â€¢ Size check      â”‚
â”‚  â€¢ Read content    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Extract text      â”‚
â”‚  â€¢ PDF â†’ text      â”‚
â”‚  â€¢ DOCX â†’ text     â”‚
â”‚  â€¢ TXT â†’ text      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk text        â”‚
â”‚  â€¢ Size: 500 char  â”‚
â”‚  â€¢ Overlap: 100    â”‚
â”‚  â€¢ Max: 100 chunks â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate          â”‚
â”‚  embeddings        â”‚
â”‚  (Gemini API)      â”‚
â”‚  3072 dimensions   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch upsert      â”‚
â”‚  to Pinecone       â”‚
â”‚  â€¢ Retry logic     â”‚
â”‚  â€¢ 25 per batch    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Success! âœ…
```

#### 2. Query Flow

```
User asks question
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate query    â”‚
â”‚  embedding         â”‚
â”‚  (Gemini API)      â”‚
â”‚  3072 dimensions   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Search Pinecone   â”‚
â”‚  â€¢ Cosine          â”‚
â”‚    similarity      â”‚
â”‚  â€¢ Top K results   â”‚
â”‚  â€¢ Return metadata â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build context     â”‚
â”‚  from top matches  â”‚
â”‚  â€¢ Combine chunks  â”‚
â”‚  â€¢ Prepare prompt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate answer   â”‚
â”‚  (Gemini 2.5)      â”‚
â”‚  â€¢ Context-aware   â”‚
â”‚  â€¢ Grounded        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Return response   â”‚
â”‚  â€¢ Answer text     â”‚
â”‚  â€¢ Source docs     â”‚
â”‚  â€¢ Similarity      â”‚
â”‚    scores          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Success! âœ…
```

### Component Details

#### FastAPI Application Layer

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Application             â”‚
â”‚                                         â”‚
â”‚  Lifespan Management:                   â”‚
â”‚  â€¢ Initialize Pinecone client           â”‚
â”‚  â€¢ Configure Gemini API                 â”‚
â”‚  â€¢ Setup logging                        â”‚
â”‚  â€¢ Graceful shutdown                    â”‚
â”‚                                         â”‚
â”‚  Middleware:                            â”‚
â”‚  â€¢ CORS (Cross-Origin Resource Sharing) â”‚
â”‚  â€¢ Rate Limiting (SlowAPI)              â”‚
â”‚  â€¢ Error Handling                       â”‚
â”‚  â€¢ Request Logging                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Vector Storage (Pinecone)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Pinecone Vector Index         â”‚
â”‚                                     â”‚
â”‚  Index: "gemini-rag"                â”‚
â”‚  Dimensions: 3072                   â”‚
â”‚  Metric: Cosine Similarity          â”‚
â”‚                                     â”‚
â”‚  Vector Structure:                  â”‚
â”‚  {                                  â”‚
â”‚    id: "filename_chunk_hash",       â”‚
â”‚    values: [3072 floats],           â”‚
â”‚    metadata: {                      â”‚
â”‚      text: "chunk content...",      â”‚
â”‚      filename: "doc.pdf",           â”‚
â”‚      chunk_index: 0                 â”‚
â”‚    }                                â”‚
â”‚  }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### AI Models (Google Gemini)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Gemini Embedding Model        â”‚
â”‚                                      â”‚
â”‚  Model: gemini-embedding-001         â”‚
â”‚  Output: 3072-dimensional vector     â”‚
â”‚  Task Type: retrieval_document       â”‚
â”‚  Use: Convert text to embeddings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Gemini Generation Model         â”‚
â”‚                                      â”‚
â”‚  Model: gemini-2.5-flash             â”‚
â”‚  Use: Generate contextual answers    â”‚
â”‚  Features:                           â”‚
â”‚  â€¢ Fast inference                    â”‚
â”‚  â€¢ Context-aware                     â”‚
â”‚  â€¢ Grounded responses                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ How It Works

### Understanding RAG (Retrieval-Augmented Generation)

RAG combines information retrieval with AI text generation to provide accurate, grounded answers:

1. **Indexing Phase** (Upload):
   ```
   Document â†’ Split into chunks â†’ Generate embeddings â†’ Store in vector DB
   ```

2. **Retrieval Phase** (Query):
   ```
   Question â†’ Generate embedding â†’ Find similar chunks â†’ Retrieve context
   ```

3. **Generation Phase** (Answer):
   ```
   Context + Question â†’ LLM â†’ Grounded answer + sources
   ```

### Why RAG?

- âœ… **Accurate**: Answers based on your documents, not hallucinations
- âœ… **Traceable**: Every answer includes source documents
- âœ… **Up-to-date**: Update knowledge by uploading new documents
- âœ… **Private**: Your data stays in your control

### Similarity Scoring

The system uses **cosine similarity** to measure relevance:

```python
Score Range: 0.0 to 1.0

0.9 - 1.0  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Excellent match
0.8 - 0.9  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   Very good match
0.7 - 0.8  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     Good match
0.6 - 0.7  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       Fair match
< 0.6      â–ˆâ–ˆâ–ˆâ–ˆ         Weak match
```

**Example from your test:**
```json
{
  "answer": "The system uses Pinecone for vector storage.",
  "sources": [
    {"filename": "test_document.txt", "score": 0.8104},  // 81% relevant
    {"filename": "test_document.txt", "score": 0.7914}   // 79% relevant
  ]
}
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or 3.12 (Python 3.13 has limited package support)
- Pinecone account ([sign up](https://app.pinecone.io/))
- Google AI Studio account ([get API key](https://aistudio.google.com/app/apikey))

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd rag-api
   ```

2. **Create virtual environment**
   ```bash
   python3.11 -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   ```

   Edit `.env` and add your API keys:
   ```env
   PINECONE_API_KEY=your_pinecone_key_here
   GEMINI_API_KEY=your_gemini_key_here
   PINECONE_INDEX=gemini-rag
   ```

5. **Create Pinecone index**

   Go to [Pinecone Console](https://app.pinecone.io/):
   - Click "Create Index"
   - Name: `gemini-rag`
   - Dimensions: `3072` âš ï¸ **CRITICAL**
   - Metric: `cosine`
   - Click "Create"

6. **Run the server**
   ```bash
   uvicorn main:app --reload
   ```

7. **Test the API**
   ```bash
   python test_api.py
   ```

   You should see:
   ```
   âœ… All tests completed!
   Your API is ready for deployment to Render!
   ```

8. **Access API documentation**

   Open your browser: [http://localhost:8000/docs](http://localhost:8000/docs)

## ğŸ“¡ API Documentation

### Base URL

```
Local: http://localhost:8000
Production: https://your-app.onrender.com
```

### Endpoints

#### 1. Health Check

```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "total_vectors": 150,
  "message": "All systems operational"
}
```

**Rate Limit:** 30 requests/minute

---

#### 2. Upload Document

```http
POST /upload
Content-Type: multipart/form-data
```

**Parameters:**
- `file` (required): Document file (.txt, .pdf, .docx)

**Example:**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "message": "File processed successfully",
  "chunks_added": 15,
  "filename": "document.pdf"
}
```

**Constraints:**
- Max file size: 2MB
- Supported formats: `.txt`, `.pdf`, `.docx`
- Max chunks per file: 100
- Chunk size: 500 characters
- Chunk overlap: 100 characters

**Rate Limit:** 5 requests/minute

---

#### 3. Query Knowledge Base

```http
POST /query
Content-Type: application/json
```

**Request Body:**
```json
{
  "question": "What is the main topic?",
  "top_k": 3
}
```

**Parameters:**
- `question` (required): Your question as a string
- `top_k` (optional): Number of results to retrieve (1-10, default: 3)

**Example:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the key features?",
    "top_k": 3
  }'
```

**Response:**
```json
{
  "answer": "Based on the documents, the key features are...",
  "sources": [
    {
      "filename": "document.pdf",
      "score": 0.8542,
      "chunk_index": 0
    },
    {
      "filename": "document.pdf",
      "score": 0.8123,
      "chunk_index": 1
    }
  ]
}
```

**Rate Limit:** 20 requests/minute

---

#### 4. Clear Database

```http
DELETE /clear
```

âš ï¸ **Warning:** This deletes ALL vectors from the database!

**Response:**
```json
{
  "message": "Database cleared successfully"
}
```

**Rate Limit:** 2 requests/minute

---

### Error Responses

All errors follow this format:

```json
{
  "detail": "Error message here"
}
```

**Common HTTP Status Codes:**
- `200` - Success
- `400` - Bad Request (invalid input)
- `404` - Not Found
- `500` - Internal Server Error
- `504` - Gateway Timeout (operation took too long)
- `429` - Too Many Requests (rate limit exceeded)

## ğŸŒ Deployment

### Deploy to Render (Free Tier)

#### Step 1: Prepare Your Code

1. Push code to GitHub:
   ```bash
   git init
   git add .
   git commit -m "Initial commit"
   git remote add origin <your-github-repo-url>
   git push -u origin main
   ```

2. Make sure `.env` is in `.gitignore` (already configured)

#### Step 2: Create Render Account

Sign up at [render.com](https://render.com)

#### Step 3: Create Web Service

1. Click "New +" â†’ "Web Service"
2. Connect your GitHub repository
3. Configure:

   | Setting | Value |
   |---------|-------|
   | **Name** | `rag-api` |
   | **Region** | Choose closest to you |
   | **Branch** | `main` |
   | **Runtime** | `Python 3` |
   | **Build Command** | `pip install -r requirements.txt` |
   | **Start Command** | `uvicorn main:app --host 0.0.0.0 --port $PORT` |
   | **Instance Type** | `Free` |

#### Step 4: Set Environment Variables

In Render dashboard, add these environment variables:

| Variable | Value |
|----------|-------|
| `PINECONE_API_KEY` | Your Pinecone API key |
| `GEMINI_API_KEY` | Your Gemini API key |
| `PINECONE_INDEX` | `gemini-rag` |
| `PYTHON_VERSION` | `3.11.0` |

#### Step 5: Deploy

Click "Create Web Service" - Render will:
1. Clone your repository
2. Install dependencies
3. Start your application
4. Give you a URL: `https://your-app.onrender.com`

#### Step 6: Test Production

```bash
# Upload a document
curl -X POST "https://your-app.onrender.com/upload" \
  -F "file=@test.txt"

# Query
curl -X POST "https://your-app.onrender.com/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "test question"}'
```

### Render Free Tier Limits

- âœ… 512 MB RAM
- âœ… Shared CPU
- âœ… 750 hours/month
- âš ï¸ App sleeps after 15 min inactivity
- âš ï¸ Cold start: ~30 seconds

### Keep Your App Awake (Optional)

Use a cron job service like [cron-job.org](https://cron-job.org):
- Ping `https://your-app.onrender.com/health` every 10 minutes
- Prevents cold starts

## âš™ï¸ Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `PINECONE_API_KEY` | Yes | - | Your Pinecone API key |
| `GEMINI_API_KEY` | Yes | - | Your Google Gemini API key |
| `PINECONE_INDEX` | No | `gemini-rag` | Name of your Pinecone index |

### Performance Tuning

Edit these constants in `main.py`:

```python
# File processing limits
MAX_FILE_SIZE_MB = 2          # Maximum upload size
CHUNK_SIZE = 500              # Characters per chunk
CHUNK_OVERLAP = 100           # Overlap between chunks
MAX_CHUNKS_PER_FILE = 100     # Max chunks to process

# API settings
BATCH_SIZE = 25               # Vectors per batch upload
TIMEOUT_SECONDS = 15          # API call timeout

# Rate limits (requests per minute)
@limiter.limit("30/minute")   # Health endpoint
@limiter.limit("5/minute")    # Upload endpoint
@limiter.limit("20/minute")   # Query endpoint
@limiter.limit("2/minute")    # Clear endpoint
```

### For Higher Traffic

If you need to handle more requests:

1. **Upgrade Render tier** ($7/month for 512MB, $25/month for 2GB)
2. **Increase rate limits** in code
3. **Add Redis** for distributed rate limiting
4. **Use load balancer** for multiple instances

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. Dimension Mismatch Error

```
Vector dimension 3072 does not match the dimension of the index 768
```

**Solution:** Delete and recreate your Pinecone index with 3072 dimensions.

---

#### 2. Model Not Found Error

```
404 models/gemini-embedding-001 is not found
```

**Solution:** Run `python test_models.py` to see available models for your API key.

---

#### 3. Import Error

```
ModuleNotFoundError: No module named 'google.generativeai'
```

**Solution:**
```bash
pip install google-generativeai==0.8.3
```

---

#### 4. Timeout Errors

```
504: Embedding generation timed out
```

**Solution:**
- Reduce `CHUNK_SIZE` to 400
- Increase `TIMEOUT_SECONDS` to 30
- Use smaller documents

---

#### 5. Rate Limit Exceeded

```
429: Too Many Requests
```

**Solution:**
- Wait 1 minute before retrying
- Reduce request frequency
- Check Gemini API quotas

---

#### 6. Memory Issues (Render Free Tier)

**Solution:**
```python
MAX_FILE_SIZE_MB = 1     # Reduce from 2
CHUNK_SIZE = 400         # Reduce from 500
BATCH_SIZE = 10          # Reduce from 25
```

## ğŸ“Š Performance

### Benchmarks (Render Free Tier)

| Metric | Value |
|--------|-------|
| Upload (1MB PDF) | ~8-12 seconds |
| Query Response | ~2-4 seconds |
| Cold Start | ~30 seconds |
| Warm Response | <1 second |
| Max Concurrent | 2-3 requests |

### Optimization Tips

1. **Use smaller chunks** for faster processing
2. **Batch uploads** instead of individual files
3. **Cache frequently asked questions**
4. **Use webhooks** instead of polling
5. **Implement request queuing** for high load

### Scaling Strategy

```
Free Tier (512MB)
  â†“
Starter ($7/mo, 512MB + more CPU)
  â†“
Standard ($25/mo, 2GB RAM)
  â†“
Pro ($85/mo, 4GB RAM)
  â†“
Custom (Redis + Load Balancer)
```

## ğŸ›¡ï¸ Security Best Practices

1. **Never commit `.env` file** - Already in `.gitignore`
2. **Use environment variables** for all secrets
3. **Update CORS origins** in production:
   ```python
   allow_origins=["https://yourdomain.com"]
   ```
4. **Add API key authentication** for public deployments
5. **Implement request signing** for sensitive data
6. **Enable HTTPS only** in production
7. **Monitor rate limits** and add IP blocking if needed

## ğŸ“ˆ Monitoring

### Logs

Check logs in Render dashboard:
1. Go to your web service
2. Click "Logs" tab
3. Monitor for errors

### Health Checks

Render automatically pings `/health` endpoint.

### Metrics to Track

- Request latency
- Error rate
- Vector count growth
- API quota usage
- Memory usage

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ†˜ Support

- **Issues**: [GitHub Issues](https://github.com/for-real-afk/SimpleRage.git)
- **Discussions**: [GitHub Discussions](https://github.com/for-real-afk/SimpleRage.git)
- **Pinecone Docs**: [docs.pinecone.io](https://docs.pinecone.io)
- **Gemini Docs**: [ai.google.dev](https://ai.google.dev)
- **Render Docs**: [render.com/docs](https://render.com/docs)

## ğŸ¯ Roadmap

- [ ] Add support for more file formats (Excel, PPT)
- [ ] Implement conversation memory
- [ ] Add user authentication
- [ ] Build web interface
- [ ] Support multiple indexes
- [ ] Add analytics dashboard
- [ ] Implement caching layer
- [ ] Support streaming responses

---

**Built with â¤ï¸ for the AI community**

Deploy your own RAG system in minutes! ğŸš€
